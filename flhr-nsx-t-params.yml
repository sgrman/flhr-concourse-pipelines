enable_ansible_debug: false # set value to true for verbose output from ansible

nsx_t_pipeline_branch: master

# format: "http://172.18.0.2"
nsx_image_webserver: "http://nginx"
ova_file_name: "nsx-unified-appliance-2.3.0.2.0.10930805.ova" #Uncomment this if downloaded file manually and placed under /home/concourse
ovftool_file_name: "VMware-ovftool-4.3.0-7948156-lin.x86_64.bundle"   #Uncomment this if downloaded file manually and placed under /home/concourse

# vCenter details to deploy the Mgmt OVAs (Mgr, Edge, Controller)
vcenter_ip: 10.0.1.251
vcenter_username: administrator@corp.local
vcenter_password: "VMware1!!"
vcenter_datacenter: FLHRNET
vcenter_cluster: MGMT01      #management cluster
vcenter_datastore: FLHRNET_NFS01

# NSX Manager general network settings
ntp_servers: 0.pool.ntp.com                           # EDIT
mgmt_portgroup: 'DVS_MGMT_ESXi_MGMT'                  # EDIT
dns_server: 10.0.1.250                                # EDIT
dns_domain: flhrnet.local                             # EDIT
default_gateway: 10.0.1.1                             # EDIT
netmask: 255.255.255.0                                # EDIT

nsx_manager_ip: 10.0.1.6
nsx_manager_username: admin
nsx_manager_password: VMware1!
nsx_manager_assigned_hostname: "nsx01.flhrnet.local"  # this hostname+dns_domain will be FQDN
nsx_manager_root_pwd: VMware1!             # Min 8 chars, upper, lower, number, special digit
nsx_manager_deployment_size: small            # Recommended for real barebones demo, smallest setup
nsx_manager_ssh_enabled: true
resource_reservation_off: true

# Compute manager credentials should be the same as above vCenter's if
# controllers and edges are to be on the same vCenter
compute_manager_username: "Administrator@vsphere.local"
compute_manager_password: "VMware1!!"
# compute manager for the compute cluster (2nd vCenter)
compute_manager_2_vcenter_ip: "null"
compute_manager_2_username: "null"
compute_manager_2_password: "null"

edge_uplink_profile_vlan: 0 # For outbound uplink connection used by Edge, usually keep as 0
esxi_uplink_profile_vlan: 200 # For internal overlay connection used by Esxi hosts, usually trasnport VLAN ID

# Virtual Tunnel Endpoint network ip pool
vtep_ip_pool_cidr: 192.168.200.0/24
vtep_ip_pool_gateway: 192.168.200.1
vtep_ip_pool_start: 192.168.200.11
vtep_ip_pool_end: 192.168.200.20

# Tier 0 router
tier0_router_name: t0-pks
tier0_uplink_port_ip: 10.0.1.252
tier0_uplink_port_subnet: 24
tier0_uplink_next_hop_ip: 10.0.1.1
tier0_uplink_port_ip_2: 10.0.1.253
tier0_ha_vip: 10.0.1.254

## Controllers
controller_ips: 10.0.1.7 #comma separated based on number of required controllers
controller_default_gateway: 10.0.1.1
controller_ip_prefix_length: 24
controller_hostname_prefix: controller01 # Generated hostname: controller_1.corp.local.io
controller_cli_password: "VMware1!" # Min 8 chars, upper, lower, num, special char
controller_root_password: "VMware1!"
controller_deployment_size: "SMALL"
vc_datacenter_for_controller: FLHRNET
vc_cluster_for_controller: MGMT01
vc_datastore_for_controller: FLHRNET_NFS01
vc_management_network_for_controller: "DVS_MGMT_ESXi_MGMT"
controller_shared_secret: "VMware1!VMware1!"

## Edge nodes
edge_ips: 10.0.1.8,10.0.1.9    #comma separated based in number of required edges
edge_default_gateway: 10.0.1.1
edge_ip_prefix_length: 24
edge_hostname_prefix: flhrnet-edge
edge_transport_node_prefix: edge-tn
edge_cli_password: "VMware1!"
edge_root_password: "VMware1!"
edge_deployment_size: "large" #Large recommended for PKS deployments
vc_datacenter_for_edge: FLHRNET
vc_cluster_for_edge: MGMT01
vc_datastore_for_edge: FLHRNET_NFS01
vc_uplink_network_for_edge: "DVS_MGMT_ESXi_MGMT"
vc_overlay_network_for_edge: "DVS_MGMT_VMs"
vc_management_network_for_edge: "DVS_MGMT_ESXi_MGMT"

## ESX hosts
#Intsll vSphere clusters automatically
clusters_to_install_nsx: COMPUTE01    #Comma seprated
per_cluster_vlans: 200  #Comma seprated, order of VLANs applied same as order of clusters

esx_ips: "" # additional esx hosts, if any, to be individually installed
esx_os_version: ""
esx_root_password: ""
esx_hostname_prefix: ""

esx_available_vmnic: "vmnic1" # comma separated physical NICs, applies to both cluster installation or ESXi installation

# Name the routers and logical switches and cidrs differently to avoid conflict
nsx_t_t1router_logical_switches_spec: |
  t1_routers:
  # Add additional T1 Routers or collapse switches into same T1 Router as needed
  # Comment off the following T1 Routers if there is no PKS
  - name: t1-pks-mgmt
    switches:
    - name: ls-pks-mgmt
      logical_switch_gw: 172.31.1.1 # Last octet should be 1 rather than 0
      subnet_mask: 24
  - name: t1-data-services
    switches:
    - name: ls-data-services
      logical_switch_gw: 172.31.2.1 # Last octet should be 1 rather than 0
      subnet_mask: 24
nsx_t_ha_switching_profile_spec: |
  ha_switching_profiles:
  - name: HASwitchingProfile
# Add additional container ip blocks as needed
nsx_t_container_ip_block_spec: |
  container_ip_blocks:
  - name: ip-block-pks-pod
    cidr: 172.16.0.0/16
  - name: ip-block-pks-nodes
    cidr: 172.15.0.0/16
# Add additional exernal ip pools as needed
nsx_t_external_ip_pool_spec: |
  external_ip_pools:
  - name: ip-pool-vips
    cidr: 10.10.0.0/24
    gateway: 10.10.0.1/24
    start: 10.10.0.12 # Should not include gateway
    end: 10.10.0.254  # Should not include gateway
# Specify NAT rules
# Provide matching dnat and snat rule for specific vms by using ips for destination_net work and translated_network that need to be exposed like Ops Mgr
# Provide snat rules for outbound from either container or a vm by specifying the source_network (cidr) and translated network ip
nsx_t_nat_rules_spec: |
  nat_rules:
  # Sample entry for PKS MGMT and Service networks
  - t0_router: t0-pks
    nat_type: snat
    source_network: 172.31.0.0/24         # PKS Clusters MGMT & SVCS network cidr
    translated_network: 10.10.0.10        # SNAT External Address for PKS networks
    rule_priority: 1024                   # Higher priority
  # Sample entry for allowing inbound to PKS Ops manager
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 10.10.0.2        # External IP address for OpsMan
    translated_network: 172.31.0.2        # Internal IP of PKS Ops manager
    rule_priority: 1024                   # Higher priority
  # Sample entry for allowing inbound to BOSH
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 10.10.0.3        # External IP address for BOSH
    translated_network: 172.31.0.3        # Internal IP of PKS Ops manager
    rule_priority: 1024                   # Higher priority
  # Sample entry for allowing inbound to PKS Controller
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 10.10.0.4       # External IP address for PKS Controller
    translated_network: 172.31.0.4       # Internal IP of PKS Ops Controller
    rule_priority: 1024                  # Higher priority
  # Sample entry for allowing inbound to Harbor
  - t0_router: t0-pks
    nat_type: dnat
    destination_network: 10.10.0.5       # External IP address for Harbor
    translated_network: 172.31.0.5       # Internal IP of Harbor
    rule_priority: 1024                  # Higher priority
nsx_t_csr_request_spec: |
  csr_request:
    #common_name not required - would use nsx_t_manager_host_name
    org_name: VMware             # EDIT
    org_unit: flhrnet            # EDIT
    country: US                  # EDIT
    state: FL                    # EDIT
    city: Cantonment             # EDIT
    key_size: 2048               # Valid values: 2048 or 3072
    algorithm: RSA               # Valid values: RSA or DSA
# LBR definition
# By default, all the server pools would use Layer 4 - TCP pass through
# No ssl termination or handling, uses default tcp active monitor & auto map for virtual server
# Auto Map mode uses LB interface IP and ephemeral port.
# In scenarios where both Clients and Pool Members are attached to the same Logical Router,
# SNAT (Auto Map or IP List) must be used.

# LBR Sizing guide
# LB Size | Virtual Servers | Pool Members
# small   |   10            |   30
# medium  |  100            |  300
# large   | 1000            | 3000

# No. of LBs per edge based on size of edge
# Edge Size | Small LBs| Medium LBs| Large LBs
#
# small     |    0     |   0       | 0
# medium    |    1     |   0       | 0 # Recommended for running only PAS or PKS
# large     |    4     |   1       | 1 # Recommended for running PAS + PKS
# Bare metal - not handled here
nsx_t_lbr_spec:
#nsx_t_lbr_spec: |
#  loadbalancers:
  # Sample entry for creating LBR for PAS ERT
#  - name: PAS-ERT-LBR
#    t1_router: T1-Router-PAS-ERT # Should match a previously declared T1 Router
#    size: small                  # Allowed sizes: small, medium, large
#    virtual_servers:
#    - name: goRouter443         # Name that signifies function being exposed
#      vip: 10.208.40.4         # Exposed VIP for LBR to listen on
#      port: 443
#      members:
#      - ip: 192.168.24.11       # Internal ip of GoRouter instance 1
#        port: 443
#    - name: goRouter80
#      vip: 10.208.40.4
#      port: 80
#      members:
#      - ip: 192.168.24.31       # Internal ip of GoRouter instance 1
#        port: 80
#      - ip: 192.168.24.32       # Internal ip of GoRouter instance 2
#        port: 80
#    - name: sshProxy            # SSH Proxy exposed to outside
#      vip: 10.208.40.5
#      port: 2222                # Port 2222 for ssh proxy
#      members:
#      - ip: 192.168.24.41       # Internal ip of Diego Brain where ssh proxy runs
#        port: 2222